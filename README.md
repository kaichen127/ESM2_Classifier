# ðŸ§¬ Post-Translational Modification (PTM) Site Prediction Model

This project uses Facebookâ€™s [ESM2](https://github.com/facebookresearch/esm) protein language model as a backbone to predict potential post-translational modifications (PTMs) in protein sequences. 

## ðŸš€ Features

- âš™ï¸ Built on top of **Facebookâ€™s ESM2** transformer model.
- ðŸ§© Supports **multi-PTM classification** through an optional embedding table and Transformer decoder head.
- ðŸ›  Adjustable configurations via configs/config.yaml including:
  - ESM model size
  - Multi-PTM embedding
  - Custom decoder layers
  - Training hyperparameters
- ðŸ“ Includes a **sample CSV dataset** from the **UniProt Consortium** for quick testing and exploration.

## ðŸ“‚ Dataset

A small example CSV dataset is provided from the [UniProt](https://www.uniprot.org/) database, including:
- Protein sequences
- Site labels

## ðŸ“ Usage

Modify configs.yaml, dataset.py, and model.py as needed and run train.py to begin training. Included in training are multiple toggleable flags that allow for loading checkpoints, visualization, and evaluating on the test set. To toggle the model's multi-PTM capabilities, change the value in configs.yaml -> model -> use_decoder_block.

## ðŸ“š Reference

- [UniProt Consortium](https://www.uniprot.org/)
- [ESM2 by Meta AI](https://github.com/facebookresearch/esm)
